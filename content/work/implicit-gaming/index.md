---
title: "Detecting Implicit Gaming through Retrospective Evaluation Sets"
date: 2023-11-23
last_mod: 2024-08-08
featured: false
reading_time: false

authors:
  - admin
  - Lucie Philippon
  - Alice Rigg
  - Cenny Wenner

summary: 2-day hackathon project, awarded first place by peer review in the Evaluations Apart Hackathon in Nov '23.

categories:
  - Research
tags:
  - Apart Research

image:
  caption: 'Figure from Detecting Implicit Gaming through Retrospective Evaluation Sets'
  preview_only: true
  filename: implicit-gaming_plot-min.jpeg

publication_types: ['manuscript']

abstract: This study introduces a methodology to detect potential implicit gaming in LLM benchmarks, where models may show inflated performance on public evaluation suites without genuine generalization. We create two retrospective evaluation sets intended to be sufficiently similar to the TruthfulQA dataset to have been its held-out samples. These sets are used to evaluate whether the gains from GPT-3 to GPT-4 models generalize. Our preliminary analysis suggests that GPT-4 may have avoided the pitfall of implicit gaming. While constrained by time, this project offers a promising approach to ensure AI advancements and safety evaluations represent genuine progress rather than dataset gaming.

url_pdf: 'https://webflow.com/files/634e78122252d2e2fc3a9ab9/formUploads/5949069b-769e-4b65-9cd5-ad9bf3f1e07d.pdf'
url_code: ''
url_dataset: ''
url_poster: ''
url_project: ''
url_slides: ''
url_source: 'https://www.apartresearch.com/project/detecting-implicit-gaming-through-retrospective-evaluation-sets'
url_video: ''

projects: []
slides: ""
---